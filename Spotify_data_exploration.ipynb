{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An exploration of the top 100 Spotify hits from 2010-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Set Up](#setup)\n",
    "    * [Import libraries](#libraries)\n",
    "    * [Set conventions](#conventions)\n",
    "    * [Make data accessibile](#access)\n",
    "\n",
    "* [Data Exploration](#explore)\n",
    "    * [Characteristics](#charas)\n",
    "    * [Display (pre-normalization)](#previs)\n",
    "        * [Notes on the histograms](#histnotes)\n",
    "    * [Feature Engineering](#featengin)\n",
    "        \n",
    "* [Predictions](#predict)\n",
    "    \n",
    "* [Normalization](#norm)\n",
    "    * [Split Data](#split)\n",
    "    * [Calculate](#calcnorm)\n",
    "        * [clipping](#clipping)\n",
    "        * [z-score](#zscore)\n",
    "    * [Display (post-normalization)](#postvis)\n",
    "        * [Notes on the violin plots](#violinnotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries <a class=\"anchor\" id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import csv\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "from ipywidgets import widgets, interactive, fixed\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set conventions <a class=\"anchor\" id=\"conventions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12,6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data accessibe<a class=\"anchor\" id=\"access\"></a>\n",
    "Dataset found on [Kaggle](https://www.kaggle.com/datasets/muhmores/spotify-top-100-songs-of-20152019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update file download parameters for outsider user\n",
    "\n",
    "# download dataset\n",
    "if not path.exists('hits_2010-2019.csv'):\n",
    "    od.download('https://www.kaggle.com/datasets/muhmores/spotify-top-100-songs-of-20152019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert to dataframe\n",
    "data = pd.read_csv('hits_2010-2019.csv', sep=',')\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration <a class=\"anchor\" id=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics <a class=\"anchor\" id=\"charas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at the shape of the data frame. This will tell us the size of the dataset we are working with. We know from the Kaggle link to expect 1000 rows (100 hits/year x 10 years) and 17 columns of data, 17000 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"column labels: \")\n",
    "for i, label in enumerate(list(df.columns)):\n",
    "    print(i, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first few rows\n",
    "# note: data sorted sequentially increasing by 'top year'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of features measured on a scale from 0-100, 0 is low and 100 is high respectively to the meaning of each feature. For more information on each feature visit the [Spotify API](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features)\n",
    "\n",
    "energy (nrgy), danceability (dnce, live (live), valence (val), acousticness (acous), speechiness (spch), popularity (pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function will only return stats for numerical data columns\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display (before normalization) <a class=\"anchor\" id=\"previs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all features we will be running analysis on\n",
    "features_list = list(df.columns)[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are mapping the spread and density of each feature. This allows us to get a general idea of the distribution of each data type. We will refer to these graphs later when prepping the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dropdown menu so we can easily switch between histograms\n",
    "def opt(_list):\n",
    "    feature = widgets.Dropdown(options=_list,\n",
    "                 value='year released',\n",
    "                 description='Feature:',\n",
    "                 disabled=False,)\n",
    "    return feature\n",
    "\n",
    "# define function for a histogram\n",
    "# graphing each feature against the year it topped the charts\n",
    "def hist2d(feature:str, df):\n",
    "    sns.histplot(data=df, x='top year', y=feature, stat='count', discrete=(True, True), cbar=True)\n",
    "    \n",
    "#     filename = f\"hist_{feature}.png\"\n",
    "#     plt.savefig(filename, format='png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the histograms\n",
    "feature = opt(features_list)\n",
    "interactive(hist2d, feature=feature, df=fixed(df))\n",
    "# plt.savefig(\"hist_feats.png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on the histograms:<a class=\"anchor\" id=\"histnotes\"></a>\n",
    "\n",
    "- The distribution of 'year released' initially does not make sense. How can a song be a chart topper in a year prior to it's release? \n",
    "    - Upon further investigation, I learned that the 'year released' column was determined by the album release \n",
    "        date. The singles that charted off the album were released ahead of time in promotion of the album. \n",
    "    - There are 75 songs with this error present.\n",
    "    \n",
    "- The 'top genre' feature is a bit of a mess to look at. But, based on the colorbar, one of the genres dominated the charts. Almost half of all songs fell into this one genre. The popularity of that genre was cut basically in half in 2017 and that trend has not returned since.\n",
    "\n",
    "- Beginnging to see small trend down of 'dur' suggesting pop songs are becoming shorter. Are attention spans wanning?\n",
    "\n",
    "- There is also a small trend down of energy. Although these trends are too small to be considered actual trends. The extent of the data is too small to draw such conclusions (only 10 years). \n",
    "\n",
    "- 'acous' and 'live' reminds us that most music is electronically produced today. Possibly though they are becoming more acoustic, meaning the interest in organic sounds is becoming popular again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering <a class=\"anchor\" id=\"featengin\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty rows\n",
    "df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'added' column, irrelevant to the goal of this project\n",
    "df.drop(labels='added', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_err.shape[0] is the number of data points with release years inconsistent with the top year\n",
    "df_err = df.loc[df['year released'] > df['top year']]\n",
    "\n",
    "# when we run the next cell, shape[0] should be zero\n",
    "print('# of inconsistent data points is', df_err.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to do two things to the 'year released' column\n",
    "\n",
    "1. Eliminate inconsistent data points. For reference, we identified 75 inconsistent data points in the 'year released' column in the above cell. \n",
    "2. Recognize that we are more interested in how many years 'year released' is offset from 'top year' than we are in the actual release date. With that in mind, let's remeasure/redefine 'year released'.\n",
    "\n",
    "Note: a negative 'year released' value now means that the song was released x years before it hit the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "df['year released'].mask(df['year released'] > df['top year'], inplace=True)\n",
    "\n",
    "# 2\n",
    "df['year released'] = df['year released'].sub(df['top year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert year type from float64 to ints\n",
    "df['top year'] = df['top year'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns that measure on a scale of 0-100 to 0-1\n",
    "\n",
    "list1 = ['nrgy', 'dnce', 'live', 'val', 'acous', 'spch', 'pop']\n",
    "df[list1] = df[list1].div(100)\n",
    "\n",
    "# return some data to verify changes have been made\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point we only care about columns of numerical measure, drop everything else\n",
    "DROP_list = ['title', 'artist', 'top genre', 'top year', 'artist type']\n",
    "df_num = df.drop(labels=DROP_list, axis=1, inplace=False)\n",
    "\n",
    "df_num = pd.DataFrame(df_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions <a class=\"anchor\" id=\"predict\"></a>\n",
    "\n",
    "The goal is create an algorithm that will forecast the features of next year's pop hits. \"Intuition\" in this case is based on the range I expected the algorithm to output within. For most of the features, I expect around 68% of the forecasting to fall in the range of the mean +/- 1 std. I will specify later on why I can't do this for all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I printed already when exploring the characteristics of the dataset\n",
    "# I'm printing it again to reminds us of the spread of the data\n",
    "df_num.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lower and upper bounds of expected output range\n",
    "LOWER = df_num.mean(axis=0, skipna=True) - df_num.std(axis=0, skipna=True)\n",
    "UPPER = df_num.mean(axis=0, skipna=True) + df_num.std(axis=0, skipna=True)\n",
    "df_predict = pd.DataFrame({'lower bound': LOWER, \n",
    "                           'upper bound': UPPER},\n",
    "                          index=list(df_num.columns))\n",
    "# print the predictions\n",
    "df_predict = df_predict.transpose()\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make corrections to upper and lower bounds based on what is logically possible\n",
    "1. Bounds of 'year released' can only be integers and must be <= 0.\n",
    "2. The column 'acous' can only be >= 0 or <= 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "df_predict.loc['upper bound', 'year released'] = 0\n",
    "\n",
    "# 2\n",
    "df_predict.loc['lower bound', 'acous'] = 0\n",
    "\n",
    "# 3 \n",
    "list2 = ['year released', 'bpm', 'dB', 'dur']\n",
    "df_predict[list1] = df_predict[list1].round(2)\n",
    "df_predict[list2] = df_predict[list2].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization <a class=\"anchor\" id=\"norm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data* <a class=\"anchor\" id=\"split\"></a>\n",
    "Data is not split at random because of the time dependency aspect of this exploration. The data size is a power of 10 so it splits evenly.\n",
    "\n",
    "70%, training set data from 2010-2016 \n",
    "\n",
    "20%, validation set data from 2017-2018\n",
    "\n",
    "10%, test set data from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df_num.columns)}\n",
    "\n",
    "n = len(df_num)\n",
    "\n",
    "# 70%\n",
    "train_df = df_num[0:int(n*0.7)]\n",
    "# 20%\n",
    "val_df = df_num[int(n*0.7):int(n*0.9)]\n",
    "# 10%\n",
    "test_df = df_num[int(n*0.9):]\n",
    "\n",
    "num_features = df_num.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate norms <a class=\"anchor\" id=\"calcnorm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clipping <a class=\"anchor\" id=\"clipping\"></a>\n",
    "Adjust for obvious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where cond False keep original value\n",
    "df_num['dB'].mask(df_num['dB'] < -12.5, inplace=True)\n",
    "df_num['dur'].mask(df_num['dur'] > 300, inplace=True)\n",
    "df_num['year released'].mask(df_num['year released'] < -5, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score <a class=\"anchor\" id=\"zscore\"></a>\n",
    "x' = (value - mean) / std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preventing bias:\n",
    "# only using training mean and std so the other values don't have access to validation and test sets\n",
    "\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display (after normalization) <a class=\"anchor\" id=\"postvis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_std = (df_num - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='numalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='numalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df_num.keys(), rotation=90)\n",
    "\n",
    "# plt.savefig(\"norm_feats.png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on the violin plots:<a class=\"anchor\" id=\"violinnotes\"></a>\n",
    "#### #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
